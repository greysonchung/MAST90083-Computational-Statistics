knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(e1071)
C <- 3
N <- 300
set.seed(50)
# Construct x, y, and z
x <- matrix(rnorm(N*2), ncol = 2)
z <- matrix(c(0, 0, 3, 0, 3, 0), C, 2)
y <- c(rep(1, 100), rep(2, 100), rep(3, 100))
# Assign class specific means to data points
x[1:100,] <- x[1:100,] + t(replicate(100, z[1,]))
x[101:200,] <- x[101:200,] + t(replicate(100, z[2,]))
x[201:300,] <- x[201:300,] + t(replicate(100, z[3,]))
x_df <- data.frame(x)
colnames(x_df) <- c('x1', 'x2')
x_df %>%
ggplot(aes(x = x1, y = x2), xlab = "x1", ylab = "x2") +
geom_point(aes(color = as.factor(y))) +
ggtitle("The three classes of X") +
scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9")) +
labs(color='Class', x = "x1", y = "x2")
tdata <- data.frame(x = x, y = as.factor(y))
svmfit <- svm(y ~., data = tdata, cost = 10, kernel = "linear")
plot(svmfit, tdata)
# Generate summary using the object svmfit
summary(svmfit)
set.seed(50)
tuned <- tune(svm, y~., data = tdata, kernel = "linear",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tuned)
bestmod <- tuned$best.model
summary(bestmod)
set.seed(10)
x_test <- matrix(rnorm(N*2), ncol=2)
set.seed(100)
y_test <- sample(c(1:3), 300, replace = TRUE)
# Assign class specific means to data points
px <- pnorm(x_test)
for (i in 1:N) {
p.1 <- px[i,1]
p.2 <- px[i,2]
if (y_test[i] == 1) {
x_test[i,1] <- qnorm(p.1, z[1,1])
x_test[i,2] <- qnorm(p.2, z[1,2])
}
else if (y_test[i] == 2) {
x_test[i,1] <- qnorm(p.1, z[2,1])
x_test[i,2] <- qnorm(p.2, z[2,2])
}
else {
x_test[i,1] <- qnorm(p.1, z[3,1])
x_test[i,2] <- qnorm(p.2, z[3,2])
}
}
testdata <- data.frame(x = x_test , y = as.factor(y_test))
set.seed(100)
y_p <- predict(bestmod, testdata)
# Print the results in form of a table for the predicted labels against the test labels
table(y_p, y_test)
svmfit_radial <- svm(y ~., data = tdata, cost = 1, gamma = 1, kernel = "radial")
plot(svmfit_radial, tdata)
# Generate summary using the object svmfit with radial kernel
summary(svmfit_radial)
set.seed(50)
# Tuning, perform a ten-fold cross-validation
svmfit_radial_cv <- tune(svm, y ~., data = tdata, kernel = "radial",
ranges = list(cost = c(0.1, 1, 10, 100, 1000),
gamma = c(0.5, 1, 2, 3, 4)))
summary(svmfit_radial_cv)
# Printing the summary of the best model with radial kernel
bestmod_radial <- svm(y ~., data = tdata, cost = 100, gamma = 2, kernel = "radial")
summary(bestmod_radial)
set.seed(100)
y_p <- predict(bestmod_radial, testdata)
# Print the results in form of a table for the predicted labels against the test labels
table(y_p, y_test)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(e1071)
C <- 3
N <- 300
set.seed(50)
# Construct x, y, and z
x <- matrix(rnorm(N*2), ncol = 2)
z <- matrix(c(0, 0, 3, 0, 3, 0), C, 2)
y <- c(rep(1, 100), rep(2, 100), rep(3, 100))
# Assign class specific means to data points
x[1:100,] <- x[1:100,] + t(replicate(100, z[1,]))
x[101:200,] <- x[101:200,] + t(replicate(100, z[2,]))
x[201:300,] <- x[201:300,] + t(replicate(100, z[3,]))
x_df <- data.frame(x)
colnames(x_df) <- c('x1', 'x2')
x_df %>%
ggplot(aes(x = x1, y = x2), xlab = "x1", ylab = "x2") +
geom_point(aes(color = as.factor(y))) +
ggtitle("The three classes of X") +
scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9")) +
labs(color='Class', x = "x1", y = "x2")
tdata <- data.frame(x = x, y = as.factor(y))
svmfit <- svm(y ~., data = tdata, cost = 10, kernel = "linear")
plot(svmfit, tdata)
# Generate summary using the object svmfit
summary(svmfit)
set.seed(50)
tuned <- tune(svm, y~., data = tdata, kernel = "linear",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tuned)
bestmod <- tuned$best.model
summary(bestmod)
set.seed(10)
x_test <- matrix(rnorm(N * 2), ncol=2)
set.seed(100)
y_test <- sample(c(1:3), 300, replace = TRUE)
# Assign class specific means to data points
for (i in 1:N) {
if (y_test[i] == 1) {
x_test[i,] <- x_test[i,] + z[1,]
}
else if (y_test[i] == 2) {
x_test[i,] <- x_test[i,] + z[2,]
}
else {
x_test[i,] <- x_test[i,] + z[3,]
}
}
testdata <- data.frame(x = x_test , y = as.factor(y_test))
set.seed(100)
y_p <- predict(bestmod, testdata)
# Print the results in form of a table for the predicted labels against the test labels
table(y_p, y_test)
svmfit_radial <- svm(y ~., data = tdata, cost = 1, gamma = 1, kernel = "radial")
plot(svmfit_radial, tdata)
# Generate summary using the object svmfit with radial kernel
summary(svmfit_radial)
set.seed(50)
# Tuning, perform a ten-fold cross-validation
svmfit_radial_cv <- tune(svm, y ~., data = tdata, kernel = "radial",
ranges = list(cost = c(0.1, 1, 10, 100, 1000),
gamma = c(0.5, 1, 2, 3, 4)))
summary(svmfit_radial_cv)
# Printing the summary of the best model with radial kernel
bestmod_radial <- svm(y ~., data = tdata, cost = 100, gamma = 2, kernel = "radial")
summary(bestmod_radial)
set.seed(100)
y_p <- predict(bestmod_radial, testdata)
# Print the results in form of a table for the predicted labels against the test labels
table(y_p, y_test)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(e1071)
C <- 3
N <- 300
set.seed(50)
# Construct x, y, and z
x <- matrix(rnorm(N*2), ncol = 2)
z <- matrix(c(0, 0, 3, 0, 3, 0), C, 2)
y <- c(rep(1, 100), rep(2, 100), rep(3, 100))
# Assign class specific means to data points
x[1:100,] <- x[1:100,] + t(replicate(100, z[1,]))
x[101:200,] <- x[101:200,] + t(replicate(100, z[2,]))
x[201:300,] <- x[201:300,] + t(replicate(100, z[3,]))
x_df <- data.frame(x)
colnames(x_df) <- c('x1', 'x2')
x_df %>%
ggplot(aes(x = x1, y = x2), xlab = "x1", ylab = "x2") +
geom_point(aes(color = as.factor(y))) +
ggtitle("The three classes of X") +
scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9")) +
labs(color='Class', x = "x1", y = "x2")
tdata <- data.frame(x = x, y = as.factor(y))
svmfit <- svm(y ~., data = tdata, cost = 10, kernel = "linear")
plot(svmfit, tdata)
# Generate summary using the object svmfit
summary(svmfit)
set.seed(50)
tuned <- tune(svm, y~., data = tdata, kernel = "linear",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tuned)
bestmod <- tuned$best.model
summary(bestmod)
set.seed(100)
x_test <- matrix(rnorm(N * 2), ncol=2)
set.seed(100)
y_test <- sample(c(1:3), 300, replace = TRUE)
# Assign class specific means to data points
for (i in 1:N) {
if (y_test[i] == 1) {
x_test[i,] <- x_test[i,] + z[1,]
}
else if (y_test[i] == 2) {
x_test[i,] <- x_test[i,] + z[2,]
}
else {
x_test[i,] <- x_test[i,] + z[3,]
}
}
testdata <- data.frame(x = x_test , y = as.factor(y_test))
set.seed(100)
y_p <- predict(bestmod, testdata)
# Print the results in form of a table for the predicted labels against the test labels
table(y_p, y_test)
svmfit_radial <- svm(y ~., data = tdata, cost = 1, gamma = 1, kernel = "radial")
plot(svmfit_radial, tdata)
# Generate summary using the object svmfit with radial kernel
summary(svmfit_radial)
set.seed(50)
# Tuning, perform a ten-fold cross-validation
svmfit_radial_cv <- tune(svm, y ~., data = tdata, kernel = "radial",
ranges = list(cost = c(0.1, 1, 10, 100, 1000),
gamma = c(0.5, 1, 2, 3, 4)))
summary(svmfit_radial_cv)
# Printing the summary of the best model with radial kernel
bestmod_radial <- svm(y ~., data = tdata, cost = 100, gamma = 2, kernel = "radial")
summary(bestmod_radial)
y_p <- predict(bestmod_radial, testdata)
# Print the results in form of a table for the predicted labels against the test labels
table(y_p, y_test)
set.seed(100)
y_p <- predict(bestmod_radial, testdata)
# Print the results in form of a table for the predicted labels against the test labels
table(y_p, y_test)
