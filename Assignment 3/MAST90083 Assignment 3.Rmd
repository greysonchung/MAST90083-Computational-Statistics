---
title: "MAST90083 Assignment 3"
author: "Haonan Zhong 867492"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1.1
```{r message=FALSE}
library(ggplot2)
library(dplyr)
library(e1071)

C <- 3
N <- 300

set.seed(50)
# Construct x, y, and z
x <- matrix(rnorm(N*2), ncol = 2)
z <- matrix(c(0, 0, 3, 0, 3, 0), C, 2)
y <- c(rep(1, 100), rep(2, 100), rep(3, 100))

# Assign class specific means to data points
x[1:100,] <- x[1:100,] + t(replicate(100, z[1,]))
x[101:200,] <- x[101:200,] + t(replicate(100, z[2,]))
x[201:300,] <- x[201:300,] + t(replicate(100, z[3,]))
```

```{r}
x_df <- data.frame(x)
colnames(x_df) <- c('x1', 'x2')

x_df %>%
  ggplot(aes(x = x1, y = x2), xlab = "x1", ylab = "x2") + 
  geom_point(aes(color = as.factor(y))) + 
  ggtitle("The three classes of X") +
  scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9")) +
  labs(color='Class', x = "x1", y = "x2")
```

## Question 1.2
```{r}
tdata <- data.frame(x = x, y = as.factor(y))
svmfit <- svm(y ~., data = tdata, cost = 10, kernel = "linear")
plot(svmfit, tdata)
```

```{r}
# Generate summary using the object svmfit
summary(svmfit)
```
As we can see from the summary output, there are a total of 67 support vectors, where 31 are from class 1, following by 19 from class 2, while 17 are from class 3.

## Question 1.3
```{r}
set.seed(50)
tuned <- tune(svm, y~., data = tdata, kernel = "linear", 
              ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tuned)
```
As we can see from the result above, the minimum cross validation error is 0.08 when the $cost=1$.

```{r}
bestmod <- tuned$best.model
summary(bestmod)
```
As the result shown, the number of support vector has increased from 67 to 81. 37 of the support vectors are from class 1, and both class 2 and class 3 has 22 support vectors.

## Question 1.4
```{r}
set.seed(100)
x_test <- matrix(rnorm(N * 2), ncol=2)

set.seed(100)
y_test <- sample(c(1:3), 300, replace = TRUE)

# Assign class specific means to data points
for (i in 1:N) {
  if (y_test[i] == 1) {
    x_test[i,] <- x_test[i,] + z[1,]
  }
  else if (y_test[i] == 2) {
    x_test[i,] <- x_test[i,] + z[2,]
  }
  else {
    x_test[i,] <- x_test[i,] + z[3,]
  }
}

testdata <- data.frame(x = x_test , y = as.factor(y_test))

set.seed(100)
y_p <- predict(bestmod, testdata)
# Print the results in form of a table for the predicted labels against the test labels
table(y_p, y_test)
```
As shown in the confusion matrix above, we can see there are 27 misclassified observations. And we can see there are 109 correctly classified observations for class 3. It is reasonable given that y is labeled randomly, as we can see there are a total number of 114 observation labeled as class 3.

## Question 1.5
```{r}
svmfit_radial <- svm(y ~., data = tdata, cost = 1, gamma = 1, kernel = "radial")
plot(svmfit_radial, tdata)
```
```{r}
# Generate summary using the object svmfit with radial kernel
summary(svmfit_radial)
```
As we can see the summary output of the SVM with radial kernel, there are a total of 94 support vectors. Where class 1, class 2, and class 3 have 39, 30, and 25, respectively.

```{r}
set.seed(50)
# Tuning, perform a ten-fold cross-validation
svmfit_radial_cv <- tune(svm, y ~., data = tdata, kernel = "radial",
                         ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                                       gamma = c(0.5, 1, 2, 3, 4)))

summary(svmfit_radial_cv)
```
As we can see from the summary above, the minimum cross validation error is 0.07333333 when $gamma=2$ and $cost=100$.

```{r}
# Printing the summary of the best model with radial kernel
bestmod_radial <- svm(y ~., data = tdata, cost = 100, gamma = 2, kernel = "radial")
summary(bestmod_radial)
```
As we can see from the summary above, the number of support vectors decreases from 94 to 88 compared to the previous model. And class 1, class 2, and class 3 have 32, 30, and 26 support vectors, respectively.

```{r}
set.seed(100)
y_p <- predict(bestmod_radial, testdata)
# Print the results in form of a table for the predicted labels against the test labels
table(y_p, y_test)
```
As the confusion matrix shown above, there are 36 observation being misclassified, which is more than the SVM fitted using linear kernel. Thus, the result here implies that radial kernel is not needed as most of the points are linearly separable.

## Question 2.1
The hidden node can be express as follow, where $f_j$ is the activation function of the hidden layer
$$
Z_j = f_j(\beta_{0j}+\sum_{m=1}^{r}\beta_{mj} x_m)
$$
The output node $Y_k$ can be express as follow, where $g_k$ is the activation function for the output layer
$$
Y_k = g_k[\alpha_{0k} + \sum_{j=1}^{t}\alpha_{jk} f_j(\beta_{0j}+\sum_{m=1}^{r}\beta_{mj} x_m)] + \epsilon_k
$$

## Question 2.2
The hidden layer can be express as follow using matrix form, where $\mathbf{f} = (f_1,...,f_t)^\top$ is the activation function of the hidden layer, $\mathbf{\beta_0} = (\beta_{01},...,\beta_{0t})^\top$ is a vector of biases of the hidden nodes, $\mathbf{B} = (\beta_{1},...,\beta_{t})^\top$ is a $(t\times r)$ matrix of connection weight for each hidden nodes
$$
\mathbf{f}(\mathbf{\beta_0} + \mathbf{B}\mathbf{X})
$$
The output layer can be express as follow using matrix form, where $\mathbf{g} = (g_1,...,g_s)^\top$ is the activation function of the hidden layer, $\mathbf{\alpha_0} = (\alpha_{01},...,\alpha_{0s})^\top$ is a vector of biases of the hidden nodes, $\mathbf{A} = (\alpha_{1},...,\alpha_{s})^\top$ is a $(s\times t)$ matrix of connection weight for each output nodes.
$$
\mathbf{\mu(X)} = \mathbf{g}(\mathbf{\alpha_0} + \mathbf{A}\mathbf{f}(\mathbf{\beta_0} + \mathbf{B}\mathbf{X}))
$$

## Question 2.3
Take $s$ as 1, therefore we will have a single output node. Let all hidden nodes in the single hidden layer to have the same sigmoidal activation function $\sigma$, and have the output activation function $g$ as linear. The network output then reduces to 
$$
y = \mu(\mathbf{x}) + \epsilon,
$$
$$
\text{where,} \; \mu(\mathbf{x}) = \alpha_{0k} + \sum_{j=1}^{t}\alpha_{jk}\sigma(\beta_{0j}+\sum_{m=1}^{r}\beta_{mj} x_m)
$$
Hence, this network becomes equivalent to a single layer perceptron.

## Question 2.4
If both activation functions for the hidden layer and output layer are taken to be identity functions, then the network simply be a linear combination of the input values, which is essentially a linear regression model.
$$
\mathbf{Y} = g(\mathbf{\alpha_0} + \mathbf{A}\mathbf{f}(\mathbf{\beta_0} + \mathbf{B}\mathbf{X})), \: \text{where} \;\mathbf{f}, \: \mathbf{g} \:\text{is the identity function}
$$
$$
\mathbf{Y} = \mathbf{\alpha_0} + \mathbf{A}\mathbf{\beta_0} + \mathbf{A}\mathbf{B}\mathbf{X}
$$
$$
\mathbf{Y} = \mu + \mathbf{A}\mathbf{B}\mathbf{X},  \: \text{where} \; \mathbf{\mu} = \mathbf{\alpha_0} + \mathbf{A}\mathbf{\beta_0}
$$


