---
title: "MAST90083 Assignment 1"
author: "Haonan Zhong"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Question 1.1
```{r}
library("MASS")
library("ISLR")
suppressMessages(library("glmnet"))
data(Hitters)
# Remove rows with NA in the Salary column
Hitters <- Hitters[!is.na(Hitters$Salary),]
```

### Question 1.2
```{r}
# Construct design matrix and response variable
x <- model.matrix(~.-1, data = subset(Hitters, select = -(Salary)))
y <- Hitters$Salary
lambda <- 10^seq(10, -2, length = 100)

# Estimate ridge coefficients for 100 lambda values
coef_estimate <- glmnet(x, y, alpha = 0, lambda = lambda)
# Observe the coefficents for the largest lambda
coef(coef_estimate)[,1]
#Observe the coefficients for the smallest lambda
coef(coef_estimate)[,100]
```
As we can see from the output above, coefficients for the largest lambda is much more closer to 0. Which is quite reasonable, as the effect of shrinkage penalty grows as $\lambda$ increases, and the ridge regression coefficients will get closer to 0.

### Question 1.3
```{r}
l2norm <- rep(0, length(lambda))
for (i in 1:100) {
  l2norm[i] <- norm(coef_estimate$beta[,i], type="2")
}
plot(log(lambda), l2norm, xlab = "Logrithm of lambda", ylab = "l2-norm of coefficients")
```

```{r}
# Computing the MSE for each lambda
mse <- rep(0, length(lambda))
for (i in 1:length(lambda)) {
  prediction <- rep(0, length(y))
  for (j in 1:length(y)) {
    prediction[j] <- t(x[j,]) %*% matrix(coef_estimate$beta[, i])
  }
  mse[i] <- mean((y - prediction)^2)
}

plot(log(lambda), mse, xlab = "Logrithm of lambda", ylab = "Mean Squared Error")
```
We cannot really say anything about the optimal value of $\lambda$ with $l2$-norm, since it only tells us the size of the coefficients. On the other hand, mean squared error can tell us how accurate the coefficients are in terms of estimating the our response variable, $Salary$.

### Question 1.4
```{r}
# Set the seed equal to 10 for random number generator
set.seed(10)
# Sample the training set
n_train <- sample(seq_len(length(y)), size = 131)
x_train <- x[n_train, ]
y_train <- y[n_train]
# Sample the testing set
x_test <- x[-n_train, ]
y_test <- y[-n_train]

# Performing 10-fold cross validation
(train_cv <- cv.glmnet(x_train, y_train, lambda = lambda, type.measure = "mse", alpha = 0))
plot(train_cv)
```
As the result above depicted, the model has the lowest mean squared error at 95,669 when $\lambda = 305.4$. Next, we will evaluate the test mean squared error.
```{r}
test_pred <- predict(train_cv$glmnet.fit, train_cv$lambda.min, newx = x_test)
mean((y_test - test_pred)^2)
```
The corresponding MSE for $\lambda = 305.4$ on the testing set is 143265.4.

```{r}
# Refit the ridge regression model on the full data set using the lambda chosen by CV
new_model <- glmnet(x, y, alpha = 0, lambda = train_cv$lambda.min)
coef(new_model)
```

```{r}
ols_model <- lm(Salary ~ ., data = Hitters)
ols_model$coefficients
```
As we can see from the output above, the coefficients of the ridge regression model are much smaller compare to the one from ordinary least square model. In some cases, the coefficients in ridge regression are shrinked close to zero, but ridge regression still retains all the variables.

### Question 1.5
```{r}
set.seed(10)
(lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, lambda = lambda, type.measure = "mse"))
plot(lasso_cv)
```

```{r}
lasso_test_pred <- predict(lasso_cv$glmnet.fit, lasso_cv$lambda.min, newx = x_test)
mean((y_test - lasso_test_pred)^2)
```
As the output above suggests, the optimal $\lambda$ value for lasso regression is 18.74, and the corresponding mean squared error for the testing set is 142270.1.

```{r}
lasso_new_model <- glmnet(x, y, alpha = 1, lambda = lasso_cv$lambda.min)
coef(lasso_new_model)
```
Finally, we refitted the lasso regression model using the $\lambda = 18.74$ selected from cross-validation. As we can see, most of the coefficients were shrinked to zero, thus, less important variables are eliminated when penalized, resulted in a sparse model compare to ordinary least square and ridge regression model.

### Question 2.1
The number of effective sample size $T$ is $n-p$.

### Question 2.2
The model can be consider as $y = X\beta + \eta$, and it can be presented as,
$$\begin{bmatrix} y_{p+1} \\ y_{p+2} \\ .\\.\\.\\y_{n} \end{bmatrix} = \begin{bmatrix} y_{p}&y_{p-1}&...& y_{1} \\ y_{p+1}&y_{p}&...& y_{2} \\ .&.&.&.\\.&.&.&.\\.&.&.&.\\y_{n-1}&y_{n-2}&...& y_{n-p} \end{bmatrix}\times\begin{bmatrix} \phi_{1} \\ \phi_{2} \\ .\\.\\.\\\phi_{p} \end{bmatrix}$$.

Therefore, to obtain the least square estimator, we can apply the normal equation $\hat{\phi} = (X^{T}X)^{-1}X^{T}y$,\newline where,
$$X = \begin{bmatrix} y_{p}&y_{p-1}&...& y_{1} \\ y_{p+1}&y_{p}&...& y_{2} \\ .&.&.&.\\.&.&.&.\\.&.&.&.\\y_{n-1}&y_{n-2}&...& y_{n-p} \end{bmatrix} \;and\; y= \begin{bmatrix} y_{p+1} \\ y_{p+2} \\ .\\.\\.\\y_{n} \end{bmatrix}$$

### Question 2.3
Given that $$ \sigma^2_{p} = \frac{RSS^2}{T} = \frac{||Y-\hat{Y}||^2}{T} $$ \newline Therefore,

$$ \sigma^2_{p} = \frac{||Y-X\hat{\phi}||^2}{n-p} = \frac{(X\hat{\phi}-Y)^T(X\hat{\phi}-Y)}{n-p} = \frac{Y^TY-Y^TX\hat{\phi}-\hat{\phi}^TX^TY + \hat{\phi}^TX^TX\hat{\phi}}{n-p}$$
where,
$$X = \begin{bmatrix} y_{p}&y_{p-1}&...& y_{1} \\ y_{p+1}&y_{p}&...& y_{2} \\ .&.&.&.\\.&.&.&.\\.&.&.&.\\y_{n-1}&y_{n-2}&...& y_{n-p} \end{bmatrix} \;and\; y= \begin{bmatrix} y_{p+1} \\ y_{p+2} \\ .\\.\\.\\y_{n} \end{bmatrix}$$

### Question 2.4
```{r}
# Here we assume the first p samples to be zero
set.seed(10)
n_samples <- 100
M1 <- rep(0, n_samples)
M2 <- rep(0, n_samples)

# Generate samples for model M1
for (i in 6:n_samples){
  M1[i] <- 0.434 * M1[i-1] + 0.217 * M1[i-2] + 0.145 * M1[i-3] + 0.108 * M1[i-4] + 
    0.087 * M1[i-5] + rnorm(1)
}

# Generate samples for model M2
for (i in 3:n_samples) {
  M2[i] <- 0.682 * M2[i-1] + 0.346 * M2[i-2] + rnorm(1)
}
```

\newpage

### Question 2.5
```{r}
# Here we will construct the response variable and the matrix X using the samples
Create_Response <- function(n, p, model) {
  y <- rep(0, n-p)
  for (t in (p+1):n) {
    y[t-p] <- model[t]
  }
  return(y)
}

Create_X <- function(n, p, model) {
  X <- matrix(0, n-p, p)
  for (i in 0:(p-1)) { 
    for (j in 1:(n-p)) {
      X[j, p-i] <- model[i+j]
    }
  }
  return(X)
}

Compute_IC1 <- function(sigma2, p, T) {
  return(log(sigma2) + (2 * (p+1)/T))
}

Compute_IC2 <- function(sigma2, p, T) {
  return(log(sigma2) + ((T + p)/(T - p - 2)))
}

Compute_IC3 <- function(sigma2, p, T) {
  return(log(sigma2) + (p * log(T)/T))
}
```

```{r}
M1_IC1 <- rep(0, 10)
M1_IC2 <- rep(0, 10)
M1_IC3 <- rep(0, 10)

M2_IC1 <- rep(0, 10)
M2_IC2 <- rep(0, 10)
M2_IC3 <- rep(0, 10)
P <- c(1:10)

for (p in P) {
  T <- 100-p
  
  M1_y <- Create_Response(n_samples, p, M1)
  M1_X <- Create_X(n_samples, p, M1)
  M1_coefficient <- solve(t(M1_X) %*% M1_X) %*% t(M1_X) %*% matrix(M1_y)
  M1_pred <- M1_X %*% M1_coefficient
  M1_sigma2 <- sum((M1[c((p+1):100)] - M1_pred)^2)/T
  
  M2_y <- Create_Response(n_samples, p, M2)
  M2_X <- Create_X(n_samples, p, M2)
  M2_coefficient <- solve(t(M2_X) %*% M2_X) %*% t(M2_X) %*% matrix(M2_y)
  M2_pred <- M2_X %*% M2_coefficient
  M2_sigma2 <- sum((M2[c((p+1):100)] - M2_pred)^2)/T

  # Compute the criteria for M1
  M1_IC1[p] <- Compute_IC1(M1_sigma2, p, T)
  M1_IC2[p] <- Compute_IC2(M1_sigma2, p, T)
  M1_IC3[p] <- Compute_IC3(M1_sigma2, p, T)
  
  # # Compute the criteria for M2
  M2_IC1[p] <- Compute_IC1(M2_sigma2, p, T)
  M2_IC2[p] <- Compute_IC2(M2_sigma2, p, T)
  M2_IC3[p] <- Compute_IC3(M2_sigma2, p, T)
}

cbind(M1_IC1, M1_IC2, M1_IC3)
cbind(M2_IC1, M2_IC2, M2_IC3)
```

```{r}
# Plotting the criteria for the M1
plot(P, M1_IC1, type = "b", pch = 19, col = 'red', ylim = c(-0.5, 1.3),
     main = "IC Values for M1", ylab = "IC")
lines(P, M1_IC2, pch = 18, col = 'blue', type = "b")
lines(P, M1_IC3, pch = 17, col = 'green', type = "b")
legend("bottomright", legend=c("IC1", "IC2", "IC3"),
       col=c("red", "blue", "Green"), lty = 1, cex=0.8)
```

```{r}
# Plotting the criteria for the M2
plot(P, M2_IC1, type = "b", pch = 19, col = 'red', ylim = c(-0.5, 1.3),
     main = "IC Values for M2", ylab = "IC")
lines(P, M2_IC2, pch = 18, col = 'blue', type = "b")
lines(P, M2_IC3, pch = 17, col = 'green', type = "b")
legend("bottomright", legend=c("IC1", "IC2", "IC3"),
       col=c("red", "blue", "Green"), lty = 1, cex=0.8)
```

