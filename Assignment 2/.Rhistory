z_matrix <- matrix(0, length(x), length(knot))
for (i in 1:length(knot)) {
for (j in 1:length(x)) {
z_matrix[j, i] <-  x[j] - knot[i]
}
}
z_matrix[ z_matrix < 0] <- 0
return(z_matrix)
}
z <- construct_basis(sort(x), k)
plot(sort(x), z[,1], type = "l", ylim = c(-1, 2), xlab = "Construction Date (Year)",
ylab = "Linear Spline Basis", main = "Figure 1a 20 Linear Spline Basis")
for (i in 2:dim(z)[2]) {
lines(sort(x), z[, i])
}
intercept <- rep(1, length(x))
Z <- construct_basis(x, k)
# Construct matrix C
C <- cbind(intercept, x, Z)
# Construct matrix D
D <- diag(1, dim(C)[2], dim(C)[2])
# Replace first two columns with 0s since we don't want to penalise them
D[, c(1, 2)] <- 0
# Tuning parameter
lambda <- seq(0, 50, length = 100)
Z
library(HRW)
data("WarsawApts")
WarsawApts<-WarsawApts[order(WarsawApts$construction.date),]
X <- WarsawApts$construction.date
y <- WarsawApts$areaPerMzloty
#Generate knots
k<-quantile(unique(X),probs = seq(0, 1, length=22)) #remove extreme value at 0 and 1 (index 1 and 22) k<-k[2:21]
k
Z<-matrix(0,nrow = length(X),ncol = 20)
generate_z<-function(Z,k){
for (i in 1:nrow(Z)){
temp<-0
for (j in 1:ncol(Z)) {
#construct (x-k)+ <0 then 0 else x-k
if ((X[i] - k[j])<0) {
temp[j]<-0
}else{
temp[j]<-X[i] - k[j]
}
}
Z[i,] <- temp
}
return(Z)
}
Z<-generate_z(Z,k)
plot(X, Z[,1], ylim=c(-1,2), xlab="construction date (year)", ylab="linear splines")
for (i in 2:20) {
lines(X,Z[,i])
}
Z
library(pracma)
library(ggplot2)
library(HRW)
data(WarsawApts)
x <- as.matrix(WarsawApts["construction.date"])
y <- as.matrix(WarsawApts["areaPerMzloty"])
n <- 409
prob=seq(0,1,length = 22)
k <- quantile(unique(x), probs = prob)[2:21]
Z <- matrix(1, ncol = 1, nrow = n)
for (i in 1:length(k)) {
Z <- cbind(Z, x-as.numeric(k[i]))
}
Z <- Z[,-1]
Z[Z<0] <- 0
Z
ones <- matrix(1, ncol=1, nrow=n)
C <- cbind(ones, x, Z)
D <- diag(1, ncol=22, nrow=22)
D[1,1] <- 0
D[2,2] <- 0
lambda <- seq(0,50,length=100)
# initialization
beta_hat <- matrix(1, ncol = 1, nrow = 22)
fitted_value <- matrix(1, ncol = 1, nrow = n)
RSS <- c()
df <- c()
GCV <- c()
for (i in 1:length(lambda)) {
beta_hat <- cbind(beta_hat,inv(t(C)%*%C+lambda[i]*D)%*%t(C)%*%y)
fitted_value <- cbind(fitted_value, C%*%(inv(t(C)%*%C+lambda[i]*D)%*%t(C)%*%y))
RSS <- c(RSS, sum((C%*%(inv(t(C)%*%C+lambda[i]*D)%*%t(C)%*%y)-y)^2))
S_lambda <- C%*%inv(t(C)%*%C+lambda[i]*D)%*%t(C)
df <- c(df, sum(diag(S_lambda)))
GCV <- c(GCV, RSS[i]/((1-df[i]/n)^2))
}
(mini=which(GCV==min(GCV)))
(lambda[mini])
beta <- inv(t(C)%*%C+lambda[mini]*D)%*%t(C)%*%y
y_fit <- C%*%beta
year <- rep(x)
origin <- rep(y)
pred <- rep(y_fit)
df <- data.frame(year = year, value1 = origin, pred = pred)
ggplot(df) +
geom_point(aes(x = year, y = value1, group = 1)) +
geom_line(aes(x = year, y = pred, group = 1)) + ylab("y")
plot(x[order(x)], Z[,1][order(x)], ylim=c(-1,2), type='l', xlab="construction date (year)", ylab="linear spline basis")
for(i in 2:20) {
lines(x[order(x)], Z[,i][order(x)], ylim=c(-1,2))
}
(MSE = RSS[mini]/n)
prob=seq(0,1,length = 62)
k <- quantile(unique(x), probs = prob)[2:61]
Z <- matrix(1, ncol = 1, nrow = n)
for (i in 1:length(k)) {
Z <- cbind(Z, x-as.numeric(k[i]))
}
Z <- Z[,-1]
Z[Z<0] <- 0
ones <- matrix(1, ncol=1, nrow=n)
C <- cbind(ones, x, Z)
D <- diag(1, ncol=62, nrow=62)
D[1,1] <- 0
D[2,2] <- 0
beta_hat <- matrix(1, ncol = 1, nrow = 62)
fitted_value <- matrix(1, ncol = 1, nrow = n)
RSS <- c()
df <- c()
GCV <- c()
for (i in 1:length(lambda)) {
beta_hat <- cbind(beta_hat,inv(t(C)%*%C+lambda[i]*D)%*%t(C)%*%y)
fitted_value <- cbind(fitted_value, C%*%(inv(t(C)%*%C+lambda[i]*D)%*%t(C)%*%y))
RSS <- c(RSS, sum((C%*%(inv(t(C)%*%C+lambda[i]*D)%*%t(C)%*%y)-y)^2))
S_lambda <- C%*%inv(t(C)%*%C+lambda[i]*D)%*%t(C)
df <- c(df, sum(diag(S_lambda)))
GCV <- c(GCV, RSS[i]/((1-df[i]/n)^2))
}
(mini=which(GCV==min(GCV)))
(lambda[mini])
RSS[mini]/n
plot(x[order(x)], Z[,1][order(x)], ylim=c(-1,2), type='l', xlab="construction date (year)", ylab="linear spline basis")
for(i in 2:60) {
lines(x[order(x)], Z[,i][order(x)], ylim=c(-1,2))
}
beta <- inv(t(C)%*%C+lambda[mini]*D)%*%t(C)%*%y
y_fit <- C%*%beta
year <- rep(x)
origin <- rep(y)
pred <- rep(y_fit)
df <- data.frame(year = year, value1 = origin, pred = pred)
ggplot(df) +
geom_point(aes(x = year, y = value1, group = 1)) +
geom_line(aes(x = year, y = pred, group = 1)) + ylab("y")
(MSE = RSS[mini]/n)
library(HRW)
data("WarsawApts")
WarsawApts<-WarsawApts[order(WarsawApts$construction.date),]
X <- WarsawApts$construction.date
y <- WarsawApts$areaPerMzloty
#Generate knots
k<-quantile(unique(X),probs = seq(0, 1, length=22)) #remove extreme value at 0 and 1 (index 1 and 22) k<-k[2:21]
k
library(pracma)
library(ggplot2)
library(HRW)
data(WarsawApts)
x <- as.matrix(WarsawApts["construction.date"])
y <- as.matrix(WarsawApts["areaPerMzloty"])
n <- 409
prob=seq(0,1,length = 22)
k <- quantile(unique(x), probs = prob)[2:21]
Z <- matrix(1, ncol = 1, nrow = n)
for (i in 1:length(k)) {
Z <- cbind(Z, x-as.numeric(k[i]))
}
Z <- Z[,-1]
Z[Z<0] <- 0
Z
C<-matrix(0,nrow = length(X),ncol = 22)
generate_C<-function(C,X,Z){
C<-cbind(rep(1,length(X)),X,Z)
return(C)
}
C<-generate_C(C,X,Z)
generate_D<-function(n){
D<-diag(1,nrow = n,ncol = n)
D[1,1]<-0
D[2,2]<-0
return(D) }
D<-generate_D(n=22)
# Create lambda
lambda <- seq(0, 50, length=100)
#Construct y_hat
y_hat<-matrix(0,nrow = length(X),ncol = length(lambda))
RSS<-rep(0,length(lambda))
df<-rep(0,length(lambda))
GVC<-rep(0,length(lambda))
#Compute y_hat, RSS, df, GVC
for (i in 1:length(lambda)) {
y_hat[,i] <- C %*% solve(t(C) %*% C + lambda[i] * C) %*% t(C) %*% y
RSS[i]<-sum((y-y_hat[,i])^2)
df[i] <- sum(diag(C %*% (solve((t(C) %*% C) + (lambda[i] * D))) %*% t(C)))
GVC[i] <- RSS[i] / (1 - ((409)^-1 * df[i]))^2
}
intercept <- rep(1, length(x))
Z <- construct_basis(x, k)
# Construct matrix C
C <- cbind(intercept, x, Z)
# Construct matrix D
D <- diag(1, dim(C)[2], dim(C)[2])
# Replace first two columns with 0s since we don't want to penalise them
D[, c(1, 2)] <- 0
# Tuning parameter
lambda <- seq(0, 50, length = 100)
library(HRW)
data(WarsawApts)
WarsawApts <- WarsawApts[order(WarsawApts$construction.date),]
x <- WarsawApts$construction.date
y <- WarsawApts$areaPerMzloty
# Exclude extreme values of 0 and 1
probVec <- seq(0, 1, length = 22)[2:21]
k <- quantile(unique(x), probs = probVec)
construct_basis <- function(x, knot) {
z_matrix <- matrix(0, length(x), length(knot))
for (i in 1:length(knot)) {
for (j in 1:length(x)) {
z_matrix[j, i] <-  x[j] - knot[i]
}
}
z_matrix[z_matrix < 0] <- 0
return(z_matrix)
}
z <- construct_basis(sort(x), k)
library(HRW)
data(WarsawApts)
WarsawApts <- WarsawApts[order(WarsawApts$construction.date),]
x <- WarsawApts$construction.date
y <- WarsawApts$areaPerMzloty
# Exclude extreme values of 0 and 1
probVec <- seq(0, 1, length = 22)[2:21]
k <- quantile(unique(x), probs = probVec)
construct_basis <- function(x, knot) {
z_matrix <- matrix(0, length(x), length(knot))
for (i in 1:length(knot)) {
for (j in 1:length(x)) {
z_matrix[j, i] <-  x[j] - knot[i]
}
}
z_matrix[z_matrix < 0] <- 0
return(z_matrix)
}
z <- construct_basis(x, k)
plot(x, z[,1], type = "l", ylim = c(-1, 2), xlab = "Construction Date (Year)",
ylab = "Linear Spline Basis", main = "Figure 1a 20 Linear Spline Basis")
for (i in 2:dim(z)[2]) {
lines(sort(x), z[, i])
}
intercept <- rep(1, length(x))
Z <- construct_basis(x, k)
# Construct matrix C
C <- cbind(intercept, x, Z)
# Construct matrix D
D <- diag(1, dim(C)[2], dim(C)[2])
# Replace first two columns with 0s since we don't want to penalise them
D[, c(1, 2)] <- 0
# Tuning parameter
lambda <- seq(0, 50, length = 100)
Z
x
k
x - k[1]
knitr::opts_chunk$set(echo = TRUE)
library(HRW)
data(WarsawApts)
WarsawApts <- WarsawApts[order(WarsawApts$construction.date),]
x <- WarsawApts$construction.date
y <- WarsawApts$areaPerMzloty
# Exclude extreme values of 0 and 1
probVec <- seq(0, 1, length = 22)[2:21]
k <- quantile(unique(x), probs = probVec)
construct_basis <- function(x, knot) {
z_matrix <- matrix(0, length(x), length(knot))
for (i in 1:length(knot)) {
for (j in 1:length(x)) {
z_matrix[j, i] <-  x[j] - knot[i]
}
}
z_matrix[z_matrix < 0] <- 0
return(z_matrix)
}
z <- construct_basis(x, k)
plot(x, z[,1], type = "l", ylim = c(-1, 2), xlab = "Construction Date (Year)",
ylab = "Linear Spline Basis", main = "Figure 1a 20 Linear Spline Basis")
for (i in 2:dim(z)[2]) {
lines(sort(x), z[, i])
}
intercept <- rep(1, length(x))
Z <- construct_basis(x, k)
# Construct matrix C
C <- cbind(intercept, x, Z)
# Construct matrix D
D <- diag(1, dim(C)[2], dim(C)[2])
# Replace first two columns with 0s since we don't want to penalise them
D[, c(1, 2)] <- 0
# Tuning parameter
lambda <- seq(0, 50, length = 100)
fit_penalised_spline <- function(X, D, lambda, y) {
RSS_error <- rep(0, length(lambda))
fitted_df <- rep(0, length(lambda))
GCV <- rep(0, length(lambda))
fitted_y <- matrix(0, length(y), length(lambda))
for (i in 1:length(lambda)) {
betaHat <- solve(t(X) %*% X + lambda[i] * D) %*% t(X) %*% y
fitted_y[, i] <- X %*% betaHat
RSS_error[i] <- sum((y - fitted_y[, i])^2)
fitted_df[i] <- sum(diag(solve(t(X) %*% X + lambda[i] * D) %*% t(X) %*% X))
GCV[i] = RSS_error[i] / (1 - fitted_df[i]/length(y))^2
}
return(list(RSS_error = RSS_error, fitted_y = fitted_y, fitted_df = fitted_df, GCV = GCV))
}
# Fit penalised spline regression for each lambda
penalised_spline_20 <- fit_penalised_spline(C, D, lambda, y)
# RSS error associated with 100 tuning parameters
(RSS_error_20 <- penalised_spline_20$RSS_error)
# Degrees of freedom associated with 100 tuning parameters
(df_20 <- penalised_spline_20$fitted_df)
# Generalised CV associated with 100 tuning parameters
(GCV_20 <- penalised_spline_20$GCV)
# Obtain lambda corresponding to the minimum GCV
min_20 <- which(GCV_20 == min(GCV_20))
(min_lambda_20 <- lambda[min_20])
# Overlay the true plot with a fit of y
plot(x, y, main = "Figure 1b Fitted Curve of 20 Basis")
lines(x[order(x)], penalised_spline_20$fitted_y[, min_20][order(x)], col = "red", lwd = 2)
# Increase the number of basis to 60
# Exclude extreme values of 0 and 1
probVec <- seq(0, 1, length = 62)[2:61]
k <- quantile(unique(x), probs = probVec)
z <- construct_basis(sort(x), k)
plot(sort(x), z[,1], typ = "l", ylim = c(-1,2), xlab = "Construction Date (Year)",
ylab = "Linear Spline Basis", main = "Figure 1c 60 Linear Spline Basis")
for (i in 2:dim(z)[2]) {
lines(sort(x), z[, i])
}
intercept <- rep(1, length(x))
Z <- construct_basis(x, k)
# Construct matrix C
C <- cbind(intercept, x, Z)
# Construct matrix D
D <- diag(1, dim(C)[2], dim(C)[2])
# Replace first two columns with 0s since we don't want to penalise them
D[, c(1, 2)] <- 0
# Fit penalised spline regression for each lambda
penalised_spline_60 <- fit_penalised_spline(C, D, lambda, y)
GCV_60 <- penalised_spline_60$GCV
# Obtain lambda corresponding to the minimum GCV
min_60 <- which(GCV_60 == min(GCV_60))
(min_lambda_60 <- lambda[min_60])
# Overlay the true plot with a fit of y
plot(x, y, main = "Figure 1d Fitted Curve of 60 Basis")
lines(x[order(x)], penalised_spline_60$fitted_y[, min_60][order(x)], col = "blue", lwd = 2)
# Compute MSE for basis 20 and 60
paste("MSE for 20 Linear Spline Basis:", RSS_error_20[min_20]/length(x))
paste("MSE for 60 Linear Spline Basis:", penalised_spline_60$RSS_error[min_60]/length(x))
# Using 6 values of k selected from 3 to 23
k <- seq(3, 23, length = 6)
KNN <- function(x, cur_x, k, y) {
abs_dif <- abs(cur_x - x)
# Sort the absolute difference and find the k nearest neighbors
indices <- sort(abs_dif, index.return = T)$ix[1:k]
# Use these indices and take their mean as the i-th estimate.
return(mean(y[indices]))
}
prediction <- matrix(0, length(y), length(k))
MSE <- rep(0, length(k))
for (i in 1:length(k)) {
y_hat <- rep(0, length(y))
count <- 1
for (j in x) {
y_hat[count] <- KNN(x, j, k[i], y)
count <- count + 1
}
prediction[, i] <- y_hat
MSE[i] <- sum((y - y_hat)^2)/length(y)
}
par(mfrow=c(3,2))
for (i in 1:length(k)) {
plot(x, y, main = paste("KNN fit with K =", k[i]))
lines(x[order(x)], prediction[, i][order(x)], col = "red", lwd = 2)
}
# Obtain k corresponding to the minimum MSE
rbind(k, MSE)
kernel_name <- c('epanechnikov', 'gaussian', 'biweight', 'triweight', 'uniform', 'tricube')
# Function that accommodate all six kernels
kernel_collection <- function(x) {
gaussian <- (2 * pi)^(-1/2) * exp(-(x^2)/2)
if (abs(x) < 1) {
epanechnikov <- (3/4) * (1 - x^2)
biweight <- (15/16) * (1 - x^2)^2
triweight <- (35/32) * (1 - x^2)^3
uniform <- 1/2
tricube <- (70/81) * (1 - abs(x)^3)^3
}
else {
epanechnikov <- biweight <- triweight <- uniform <- tricube <- 0
}
return(c(epanechnikov, gaussian, biweight, triweight, uniform, tricube))
}
# Bandwidth
h <- 2
kernel_regression <- function(x, y) {
prediction <- matrix(0, length(y), 6)
for (i in 1:length(y)) {
weight <- matrix(0, length(y), 6)
nw <- 0
for (j in 1:length(y)) {
weight[j, ] = kernel_collection((x[j] - x[i])/h)
nw <- nw + (weight[j, ] * y[j])
}
prediction[i,] <-  nw / colSums(weight)
}
colnames(prediction) <- kernel_name
return(prediction)
}
prediction <- kernel_regression(x, y)
par(mfrow=c(3,2))
for (i in 1:6) {
plot(x, y, main = paste("Kernel", kernel_name[i]))
lines(x[order(x)], prediction[, i][order(x)], col = "red", lwd = 2)
}
# Estimate the MSE for each kernel
colSums(((y - prediction)^2)/length(y))
# Load libraries and image
suppressMessages(library(plot.matrix))
suppressMessages(library(png))
suppressMessages(library(fields))
I <- readPNG("CM.png")
I <- I[,, 1]
I <- t(apply(I, 2, rev))
par(mfrow=c(1, 2))
image(I, col = gray((0:255) / 255))
plot(density(I))
compute.prob.x.z <- function(X, pi.curr, mu.curr, sigma.curr) {
L <- matrix(NA, nrow = length(X), ncol = length(pi.curr))
for (k in seq_len(ncol(L))) {
L[, k] <- dnorm(X, mean = mu.curr[k], sd = sigma.curr[k]) * pi.curr[k]
}
return(list(p = L))
}
EM.iter <- function(X, pi.curr, mu.curr, sigma.curr) {
# Expectation step
p <- compute.prob.x.z(X, pi.curr, mu.curr, sigma.curr)$p
# Compute the responsibility
P_ik <- p / rowSums(p)
# Maximisation step
pi.new <- colSums(P_ik) / sum(P_ik)
mu.new <- colSums(P_ik*X) / colSums(P_ik)
temp <- P_ik * (replicate(length(mu.new), X) - t(replicate(length(X), mu.new)))^2
sigma.new <- sqrt(colSums(temp)/colSums(P_ik))
return(list(pi.new = pi.new, mu.new = mu.new, sigma.new = sigma.new))
}
EM <- function(X, p, mu, sigma, epsilon = 1e-6, max.iter=100) {
p.curr <- p
mu.curr <- mu
sigma.curr <- sigma
e.curr <- 0
delta <- 1
count <- 1
while ((delta > epsilon)) {
# run EM step to compute new values for the parameters
EM.out <- EM.iter(X, p.curr, mu.curr, sigma.curr)
count <- count + 1
# print the new estimates
print(round(c(EM.out$mu.new, EM.out$sigma.new, EM.out$pi.new), 4))
# compute the stopping criteria
e.new <- sum(EM.out$mu.new - mu.curr) +
sum(EM.out$sigma.new - sigma.curr)
delta <- abs(e.new - e.curr)
# replace current values with new estimates
p.curr <- EM.out$pi.new
mu.curr <- EM.out$mu.new
sigma.curr <- EM.out$sigma.new
e.curr <- e.new
}
return(list(pi = EM.out$pi.new, mu = EM.out$mu.new, sigma = EM.out$sigma.new))
}
# Define the initial values for the mixture model
p.init <- c(0.2, 0.3, 0.5)
mu.init <- c(0, 0.4, 0.7)
sigma.init <- c(0.2, 0.2, 0.2)
# Run EM
result <- EM(as.vector(I), p.init, mu.init, sigma.init)
# Compute the probability of each pixels being in each class
predict_prob <- compute.prob.x.z(as.vector(I), result$pi, result$mu, result$sigma)
# Classify each pixels based on their pdf estimation
label <- rep(0, length(as.vector(I)))
for (i in 1:length(as.vector(I))) {
label[i] <- which(predict_prob$p[i, ] == max(predict_prob$p[i, ]))
}
posterior <- predict_prob$p / rowSums(predict_prob$p)
mean <- t(replicate(length(as.vector(I)), result$mu))
posterior.mean <- rowSums(posterior * mean)
# Assign values based on figure 3a
label[label == 1] <- 0
label[label == 2] <- 8
label[label == 3] <- 4
# Check dimension for reshape
dim(I)
# Plot figure 3a
image.plot(matrix(label, 398, 398))
# Plot figure 3b
image.plot(matrix(posterior.mean, 398, 398))
# Obtain lambda corresponding to the minimum GCV
min_20 <- which(GCV_20 == min(GCV_20))
(min_lambda_20 <- lambda[min_20])
