---
title: "MAST90083 Assignment 2"
author: "Haonan Zhong 867492"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1.1
```{r}
library(HRW)
data(WarsawApts)
# Reorder the dataset base on x
WarsawApts <- WarsawApts[order(WarsawApts$construction.date), ]

x <- WarsawApts$construction.date
y <- WarsawApts$areaPerMzloty

# Exclude extreme values of 0 and 1
probVec <- seq(0, 1, length = 22)[2:21]
k <- quantile(unique(x), probs = probVec)
```

## Question 1.2
```{r}
construct_basis <- function(x, knot) {
  z_matrix <- matrix(0, length(x), length(knot))
  for (i in 1:length(knot)) {
    for (j in 1:length(x)) {
      z_matrix[j, i] <-  x[j] - knot[i]
    }
  }
  z_matrix[z_matrix < 0] <- 0
  return(z_matrix)
}

z <- construct_basis(sort(x), k)
```

```{r}
plot(x, z[,1], type = "l", ylim = c(-1, 2), xlab = "Construction Date (Year)",
     ylab = "Linear Spline Basis", main = "Figure 1a 20 Linear Spline Basis")
for (i in 2:20) {
  lines(x, z[, i]) 
}
```

## Question 1.3
```{r}
intercept <- rep(1, length(x))
Z <- construct_basis(x, k)
# Construct matrix C
C <- cbind(intercept, x, Z)

# Construct matrix D
D <- diag(1, dim(C)[2], dim(C)[2])
# Replace first two columns with 0s since we don't want to penalise them
D[, c(1, 2)] <- 0

# Tuning parameter
lambda <- seq(0, 50, length = 100)
```

## Question 1.4
```{r}
fit_penalised_spline <- function(X, D, lambda, y) {
  RSS_error <- rep(0, length(lambda))
  fitted_df <- rep(0, length(lambda))
  GCV <- rep(0, length(lambda))
  fitted_y <- matrix(0, length(y), length(lambda))
  
  for (i in 1:length(lambda)) {
    betaHat <- solve(t(X) %*% X + lambda[i] * D) %*% t(X) %*% y
    fitted_y[, i] <- X %*% betaHat
    RSS_error[i] <- sum((y - fitted_y[, i])^2)
    fitted_df[i] <- sum(diag(solve(t(X) %*% X + lambda[i] * D) %*% t(X) %*% X))
    GCV[i] = RSS_error[i] / (1 - fitted_df[i]/length(y))^2
  }
  
  return(list(RSS_error = RSS_error, fitted_y = fitted_y, fitted_df = fitted_df, GCV = GCV))
}
```

```{r}
# Fit penalised spline regression for each lambda
penalised_spline_20 <- fit_penalised_spline(C, D, lambda, y)

# RSS error associated with 100 tuning parameters
(RSS_error_20 <- penalised_spline_20$RSS_error)

# Degrees of freedom associated with 100 tuning parameters
(df_20 <- penalised_spline_20$fitted_df)

# Generalised CV associated with 100 tuning parameters
(GCV_20 <- penalised_spline_20$GCV)
```

## Question 1.5
```{r}
# Obtain lambda corresponding to the minimum GCV
min_20 <- which(GCV_20 == min(GCV_20))
(min_lambda_20 <- lambda[min_20])
```

```{r}
# Overlay the true plot with a fit of y
plot(x, y, main = "Figure 1b Fitted Curve of 20 Basis")
lines(x, penalised_spline_20$fitted_y[, min_20], col = "red", lwd = 2)
```

```{r}
# Increase the number of basis to 60
# Exclude extreme values of 0 and 1
probVec <- seq(0, 1, length = 62)[2:61]
k <- quantile(unique(x), probs = probVec)

z <- construct_basis(x, k)

plot(x, z[,1], typ = "l", ylim = c(-1,2), xlab = "Construction Date (Year)", 
     ylab = "Linear Spline Basis", main = "Figure 1c 60 Linear Spline Basis")
for (i in 2:60) {
  lines(x, z[, i]) 
}
```

```{r}
intercept <- rep(1, length(x))
Z <- construct_basis(x, k)
# Construct C matrix C
C <- cbind(intercept, x, Z)

# Construct matrix D
D <- diag(1, dim(C)[2], dim(C)[2])
# Replace first two columns with 0s since we don't want to penalise them
D[, c(1, 2)] <- 0
# Fit penalised spline regression for each lambda
penalised_spline_60 <- fit_penalised_spline(C, D, lambda, y)
GCV_60 <- penalised_spline_60$GCV

# Obtain lambda corresponding to the minimum GCV
min_60 <- which(GCV_60 == min(GCV_60))
(min_lambda_60 <- lambda[min_60])
```

```{r}
# Overlay the true plot with a fit of y
plot(x, y, main = "Figure 1d Fitted Curve of 60 Basis")
lines(x, penalised_spline_60$fitted_y[, min_60], col = "blue", lwd = 2)
```

```{r}
# Compute MSE for basis 20 and 60
paste("MSE for 20 Linear Spline Basis:", RSS_error_20[min_20]/length(x))
paste("MSE for 60 Linear Spline Basis:", penalised_spline_60$RSS_error[min_60]/length(x))
```
As we can see from Figure 1d. the fitted curve for 60 linear spline basis appears to be more smooth compared to the one for 20 linear spline basis. However, from the computed mean squared square errors, we cannot see an improvement for the 60 spline basis, and it is even slight higher than the mean square error of the 20 spline basis.

## Question 2.1
```{r}
# Using 6 values of k selected from 3 to 23
k <- seq(3, 23, length = 6)

KNN <- function(x, cur_x, k, y) {
  abs_dif <- abs(cur_x - x)
  
  # Sort the absolute difference and find the k nearest neighbors
  indices <- sort(abs_dif, index.return = TRUE)$ix[c(1:k)]
  
  # Use these indices and take their mean as the i-th estimate.
  return(mean(y[indices]))
}
```

## Question 2.2
```{r}
prediction <- matrix(0, length(y), length(k))
MSE <- rep(0, length(k))

for (i in 1:length(k)) {
  y_hat <- rep(0, length(y))
  count <- 1
  for (j in x) {
    y_hat[count] <- KNN(x, j, k[i], y)
    count <- count + 1
  }
  prediction[, i] <- y_hat
  # Compute the mean square error for each k
  MSE[i] <- sum((y - y_hat)^2)/length(y)
}
```

```{r}
par(mfrow=c(3,2))

for (i in 1:length(k)) {
  plot(x, y, main = paste("KNN fit with K =", k[i]))
  lines(x, prediction[, i], col = "red", lwd = 2)
}
```

```{r}
# Obtain k corresponding to the minimum MSE
rbind(k, MSE)
```
As we can see from the table, the minimum mean squared error is 293.9181 when $K=7$, which is quite similar to the one we obtained using penalised spline regression, but slightly lower. And we can see that the MSE increases as k increases.

## Question 2.3
```{r}
kernel_name <- c('epanechnikov', 'gaussian', 'biweight', 'triweight', 'uniform', 'tricube')
# Function that accommodate all six kernels
kernel_collection <- function(x) {
  gaussian <- (2 * pi)^(-1/2) * exp(-(x^2)/2)
  
  if (abs(x) < 1) {
    epanechnikov <- (3/4) * (1 - x^2)
    biweight <- (15/16) * (1 - x^2)^2
    triweight <- (35/32) * (1 - x^2)^3
    uniform <- 1/2
    tricube <- (70/81) * (1 - abs(x)^3)^3
  }
  else {
    epanechnikov <- biweight <- triweight <- uniform <- tricube <- 0
  }
  return(c(epanechnikov, gaussian, biweight, triweight, uniform, tricube))
}
```

## Question 2.4
```{r}
# Bandwidth
h <- 2

kernel_regression <- function(x, y) {
  prediction <- matrix(0, length(y), 6)
  
  for (i in 1:length(y)) {
    weight <- matrix(0, length(y), 6)
    nw <- 0
    for (j in 1:length(y)) {
      weight[j, ] = kernel_collection((x[j] - x[i])/h)
      nw <- nw + (weight[j, ] * y[j])
    }
    prediction[i,] <-  nw / colSums(weight)
  }
  colnames(prediction) <- kernel_name
  return(prediction)
}
```

## Question 2.5
```{r}
prediction <- kernel_regression(x, y)
par(mfrow=c(3,2))

for (i in 1:6) {
  plot(x, y, main = paste("Kernel", kernel_name[i]))
  lines(x, prediction[, i], col = "red", lwd = 2)
}
```

```{r}
# Estimate the MSE for each kernel
colSums(((y - prediction)^2)/length(y))
```
As we can see, triweight kernel gives the lowest mean square error at 272.9163.

## Question 3.1
```{r message=FALSE}
# Load libraries and image
library(plot.matrix)
library(png)
library(fields)

I <- readPNG("CM.png")
I <- I[,, 1]
I <- t(apply(I, 2, rev))

par(mfrow=c(1, 2))
image(I, col = gray((0:255) / 255))
plot(density(I))
```

## Question 3.2 and 3.3
Here the expectation step will be computing
$$
P(Z_i=k|X_i) = \frac{P(Z_i=k)P(X_i|Z_i=k)}{\sum_{k=1}^{K}P(Z_i=k)P(X_i|Z_i=k)}
$$
The maximisation step will be updating the parameter estimates base on the following equations
$$
\hat{\pi_k} = \frac{\sum_{i=1}^{N}P(Z_i=k|X_i)}{N} \; \; \;
\hat{\mu}_k= \frac{\sum_{i=1}^{N}P(Z_i=k|X_i)\cdot x_i}{\sum_{i=1}^{N}P(Z_i=k|X_i)} \;\;\;
\hat{\sigma_{k}^{2}} = \frac{\sum_{i=1}^{N}P(Z_i=k|X_i)(x_i-\hat{\mu}_k)^2}{\sum_{i=1}^{N}P(Z_i=k|X_i)}
$$

```{r}
compute.posterior <- function(X, p.curr, mu.curr, sigma.curr) {
  # compute the posterior of the latent variable given the current estimates and data
  p<- matrix(NA, nrow = length(X), ncol = length(p.curr))
  for (k in 1:length(p.curr)) {
    p[, k] <- p.curr[k] * dnorm(X, mean = mu.curr[k], sd = sigma.curr[k])
  }
  return(list(p = p))
}

EM.iter <- function(X, p.curr, mu.curr, sigma.curr) {
  
  # Expectation step
  p <- compute.posterior(X, p.curr, mu.curr, sigma.curr)$p
  
  # Compute the posterior of latent variable given the observation
  gamma_i <- p / rowSums(p)
  
  # Maximisation step: compute new parameter estimates
  p.new <- colSums(gamma_i) / sum(gamma_i)
  mu.new <- colSums(gamma_i * X) / colSums(gamma_i)

  temp <- gamma_i * (replicate(length(mu.new), X) - t(replicate(length(X), mu.new)))^2
  sigma.new <- sqrt(colSums(temp) / colSums(gamma_i))
  
  return(list(p.new = p.new, mu.new = mu.new, sigma.new = sigma.new))
}

EM <- function(X, p, mu, sigma, epsilon = 1e-6) {
  p.curr <- p
  mu.curr <- mu
  sigma.curr <- sigma
  e.curr <- 0
  
  delta <- 1
  count <- 1
  
  # keep updating until convergence
  while (delta > epsilon) {
    # run EM to compute new estimates for the parameters
    EM.out <- EM.iter(X, p.curr, mu.curr, sigma.curr)
    count <- count + 1
    # print the new estimates of the current iteration
    print(round(c(EM.out$mu.new, EM.out$sigma.new, EM.out$p.new), 4))
    
    # compute the stopping criteria
    e.new <- sum(EM.out$mu.new - mu.curr) + sum(EM.out$sigma.new - sigma.curr)
    delta <- abs(e.new - e.curr)
    
    # replace current values with new estimates
    p.curr <- EM.out$p.new
    mu.curr <- EM.out$mu.new
    sigma.curr <- EM.out$sigma.new
    e.curr <- e.new
  }
  return(list(p = p.curr, mu = mu.curr, sigma = sigma.curr))
}
```

```{r}
# Define the initial values for the mixture model
p <- c(0.2, 0.3, 0.5)
mu <- c(0, 0.4, 0.7)
sigma <- c(0.2, 0.2, 0.2)
# Run EM
result <- EM(as.vector(I), p, mu, sigma)
```

## Question 3.4
```{r}
# Compute the probability of each pixels being in each class
predict_prob <- compute.posterior(as.vector(I), result$p, result$mu, result$sigma)

# Classify each pixels based on their pdf estimation
label <- rep(0, length(as.vector(I)))
for (i in 1:length(as.vector(I))) {
  label[i] <- which(predict_prob$p[i, ] == max(predict_prob$p[i, ]))
}

# Posterior PDFs multiplied with their mean
posterior <- predict_prob$p / rowSums(predict_prob$p)
mean <- t(replicate(length(as.vector(I)), result$mu))
posterior.mean <- rowSums(posterior * mean)

# Assign pixel values based on figure 3a color bar
label[label == 1] <- 0
label[label == 2] <- 8
label[label == 3] <- 4
```

## Question 3.5
```{r}
# Plot figure 3a and 3b
image.plot(matrix(label, 398, 398))
```

```{r}
image.plot(matrix(posterior.mean, 398, 398))
```

