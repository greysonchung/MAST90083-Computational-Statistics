---
title: "MAST90083 Assignment 2"
author: "Haonan Zhong"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1.1
```{r}
library(HRW)
data(WarsawApts)
WarsawApts <- WarsawApts[order(WarsawApts$construction.date),]

x <- WarsawApts$construction.date
y <- WarsawApts$areaPerMzloty

# Exclude extreme values of 0 and 1
probVec <- seq(0, 1, length = 22)[2:21]
k <- quantile(unique(x), probs = probVec)
```

## Question 1.2
```{r}
construct_basis <- function(x, knot) {
  z_matrix <- matrix(0, length(x), length(knot))
  for (i in 1:length(knot)) {
    for (j in 1:length(x)) {
      z_matrix[j, i] <-  x[j] - knot[i]
    }
  }
  z_matrix[z_matrix < 0] <- 0
  return(z_matrix)
}

z <- construct_basis(x, k)
```

```{r}
plot(x, z[,1], type = "l", ylim = c(-1, 2), xlab = "Construction Date (Year)",
     ylab = "Linear Spline Basis", main = "Figure 1a 20 Linear Spline Basis")
for (i in 2:dim(z)[2]) {
  lines(sort(x), z[, i]) 
}
```

## Question 1.3
```{r}
intercept <- rep(1, length(x))
Z <- construct_basis(x, k)
# Construct matrix C
C <- cbind(intercept, x, Z)

# Construct matrix D
D <- diag(1, dim(C)[2], dim(C)[2])
# Replace first two columns with 0s since we don't want to penalise them
D[, c(1, 2)] <- 0

# Tuning parameter
lambda <- seq(0, 50, length = 100)
```

## Question 1.4
```{r}
fit_penalised_spline <- function(X, D, lambda, y) {
  RSS_error <- rep(0, length(lambda))
  fitted_df <- rep(0, length(lambda))
  GCV <- rep(0, length(lambda))
  fitted_y <- matrix(0, length(y), length(lambda))
  
  for (i in 1:length(lambda)) {
    betaHat <- solve(t(X) %*% X + lambda[i] * D) %*% t(X) %*% y
    fitted_y[, i] <- X %*% betaHat
    RSS_error[i] <- sum((y - fitted_y[, i])^2)
    fitted_df[i] <- sum(diag(solve(t(X) %*% X + lambda[i] * D) %*% t(X) %*% X))
    GCV[i] = RSS_error[i] / (1 - fitted_df[i]/length(y))^2
  }
  
  return(list(RSS_error = RSS_error, fitted_y = fitted_y, fitted_df = fitted_df, GCV = GCV))
}
```

```{r}
# Fit penalised spline regression for each lambda
penalised_spline_20 <- fit_penalised_spline(C, D, lambda, y)

# RSS error associated with 100 tuning parameters
(RSS_error_20 <- penalised_spline_20$RSS_error)

# Degrees of freedom associated with 100 tuning parameters
(df_20 <- penalised_spline_20$fitted_df)

# Generalised CV associated with 100 tuning parameters
(GCV_20 <- penalised_spline_20$GCV)
```

## Question 1.5
```{r}
# Obtain lambda corresponding to the minimum GCV
min_20 <- which(GCV_20 == min(GCV_20))
(min_lambda_20 <- lambda[min_20])
```

```{r}
# Overlay the true plot with a fit of y
plot(x, y, main = "Figure 1b Fitted Curve of 20 Basis")
lines(x[order(x)], penalised_spline_20$fitted_y[, min_20][order(x)], col = "red", lwd = 2)
```

```{r}
# Increase the number of basis to 60
# Exclude extreme values of 0 and 1
probVec <- seq(0, 1, length = 62)[2:61]
k <- quantile(unique(x), probs = probVec)

z <- construct_basis(sort(x), k)

plot(sort(x), z[,1], typ = "l", ylim = c(-1,2), xlab = "Construction Date (Year)", 
     ylab = "Linear Spline Basis", main = "Figure 1c 60 Linear Spline Basis")
for (i in 2:dim(z)[2]) {
  lines(sort(x), z[, i]) 
}
```

```{r}
intercept <- rep(1, length(x))
Z <- construct_basis(x, k)
# Construct C matrix C
C <- cbind(intercept, x, Z)

# Construct matrix D
D <- diag(1, dim(C)[2], dim(C)[2])
# Replace first two columns with 0s since we don't want to penalise them
D[, c(1, 2)] <- 0
# Fit penalised spline regression for each lambda
penalised_spline_60 <- fit_penalised_spline(C, D, lambda, y)
GCV_60 <- penalised_spline_60$GCV

# Obtain lambda corresponding to the minimum GCV
min_60 <- which(GCV_60 == min(GCV_60))
(min_lambda_60 <- lambda[min_60])
```

```{r}
# Overlay the true plot with a fit of y
plot(x, y, main = "Figure 1d Fitted Curve of 60 Basis")
lines(x[order(x)], penalised_spline_60$fitted_y[, min_60][order(x)], col = "blue", lwd = 2)
```

```{r}
# Compute MSE for basis 20 and 60
paste("MSE for 20 Linear Spline Basis:", RSS_error_20[min_20]/length(x))
paste("MSE for 60 Linear Spline Basis:", penalised_spline_60$RSS_error[min_60]/length(x))
```
As we can see from Figure 2d. the fitted curve for 60 linear spline basis appears to be more smooth compared to the one for 20 linear spline basis. However, from the computed mean squared square errors, we cannot see an improvement for the 60 spline basis, and it is even slight higher than the mean square error of the 20 spline basis.

## Question 2.1
```{r}
# Using 6 values of k selected from 3 to 23
k <- seq(3, 23, length = 6)

KNN <- function(x, cur_x, k, y) {
  abs_dif <- abs(cur_x - x)
  
  # Sort the absolute difference and find the k nearest neighbors
  indices <- sort(abs_dif, index.return = TRUE)$ix[1:k]
  
  # Use these indices and take their mean as the i-th estimate.
  return(mean(y[indices]))
}
```

## Question 2.2
```{r}
prediction <- matrix(0, length(y), length(k))
MSE <- rep(0, length(k))

for (i in 1:length(k)) {
  y_hat <- rep(0, length(y))
  count <- 1
  for (j in x) {
    y_hat[count] <- KNN(x, j, k[i], y)
    count <- count + 1
  }
  prediction[, i] <- y_hat
  # Compute the mean square error for each k
  MSE[i] <- sum((y - y_hat)^2)/length(y)
}
```

```{r}
par(mfrow=c(3,2))

for (i in 1:length(k)) {
  plot(x, y, main = paste("KNN fit with K =", k[i]))
  lines(x[order(x)], prediction[, i][order(x)], col = "red", lwd = 2)
}
```

```{r}
# Obtain k corresponding to the minimum MSE
rbind(k, MSE)
```
As we can see from the table, the minimum mean squared error is 293.9181 when $K=7$, which is quite similar to the one we obtained using penalised spline regression, but slightly higher.

## Question 2.3
```{r}
kernel_name <- c('epanechnikov', 'gaussian', 'biweight', 'triweight', 'uniform', 'tricube')
# Function that accommodate all six kernels
kernel_collection <- function(x) {
  gaussian <- (2 * pi)^(-1/2) * exp(-(x^2)/2)
  
  if (abs(x) < 1) {
    epanechnikov <- (3/4) * (1 - x^2)
    biweight <- (15/16) * (1 - x^2)^2
    triweight <- (35/32) * (1 - x^2)^3
    uniform <- 1/2
    tricube <- (70/81) * (1 - abs(x)^3)^3
  }
  else {
    epanechnikov <- biweight <- triweight <- uniform <- tricube <- 0
  }
  return(c(epanechnikov, gaussian, biweight, triweight, uniform, tricube))
}
```

## Question 2.4
```{r}
# Bandwidth
h <- 2

kernel_regression <- function(x, y) {
  prediction <- matrix(0, length(y), 6)
  
  for (i in 1:length(y)) {
    weight <- matrix(0, length(y), 6)
    nw <- 0
    for (j in 1:length(y)) {
      weight[j, ] = kernel_collection((x[j] - x[i])/h)
      nw <- nw + (weight[j, ] * y[j])
    }
    prediction[i,] <-  nw / colSums(weight)
  }
  colnames(prediction) <- kernel_name
  return(prediction)
}
```

## Question 2.5
```{r}
prediction <- kernel_regression(x, y)
par(mfrow=c(3,2))

for (i in 1:6) {
  plot(x, y, main = paste("Kernel", kernel_name[i]))
  lines(x[order(x)], prediction[, i][order(x)], col = "red", lwd = 2)
}
```

```{r}
# Estimate the MSE for each kernel
colSums(((y - prediction)^2)/length(y))
```
As we can see, triweight kernel gives the lowest mean square error at 272.9163.

## Question 3.1
```{r}
# Load libraries and image
suppressMessages(library(plot.matrix))
suppressMessages(library(png))
suppressMessages(library(fields))

I <- readPNG("CM.png")
I <- I[,, 1]
I <- t(apply(I, 2, rev))

par(mfrow=c(1, 2))
image(I, col = gray((0:255) / 255))
plot(density(I))
```

## Question 3.2 and 3.3
```{r}
compute.prob.x.z <- function(X, pi.curr, mu.curr, sigma.curr) {
  L <- matrix(NA, nrow = length(X), ncol = length(pi.curr))
  for (k in seq_len(ncol(L))) {
    L[, k] <- dnorm(X, mean = mu.curr[k], sd = sigma.curr[k]) * pi.curr[k]
  }
  return(list(p = L))
}

EM.iter <- function(X, pi.curr, mu.curr, sigma.curr) {
  
  # Expectation step
  p <- compute.prob.x.z(X, pi.curr, mu.curr, sigma.curr)$p
  
  # Compute the responsibility
  P_ik <- p / rowSums(p)
  
  # Maximisation step
  pi.new <- colSums(P_ik) / sum(P_ik)
  mu.new <- colSums(P_ik*X) / colSums(P_ik)

  temp <- P_ik * (replicate(length(mu.new), X) - t(replicate(length(X), mu.new)))^2
  sigma.new <- sqrt(colSums(temp)/colSums(P_ik))
  
  return(list(pi.new = pi.new, mu.new = mu.new, sigma.new = sigma.new))
}

EM <- function(X, p, mu, sigma, epsilon = 1e-6, max.iter=100) {
  p.curr <- p
  mu.curr <- mu
  sigma.curr <- sigma
  e.curr <- 0
  
  delta <- 1
  count <- 1
  while ((delta > epsilon)) {
    # run EM to compute new estimates for the parameters
    EM.out <- EM.iter(X, p.curr, mu.curr, sigma.curr)
    count <- count + 1
    # print the new estimates
    print(round(c(EM.out$mu.new, EM.out$sigma.new, EM.out$pi.new), 4))
    
    # compute the stopping criteria
    e.new <- sum(EM.out$mu.new - mu.curr) +
      sum(EM.out$sigma.new - sigma.curr)
    delta <- abs(e.new - e.curr)
    
    # replace current values with new estimates
    p.curr <- EM.out$pi.new
    mu.curr <- EM.out$mu.new
    sigma.curr <- EM.out$sigma.new
    e.curr <- e.new
  }
  return(list(pi = EM.out$pi.new, mu = EM.out$mu.new, sigma = EM.out$sigma.new))
}
```

```{r}
# Define the initial values for the mixture model
p.init <- c(0.2, 0.3, 0.5)
mu.init <- c(0, 0.4, 0.7)
sigma.init <- c(0.2, 0.2, 0.2)
# Run EM
result <- EM(as.vector(I), p.init, mu.init, sigma.init)
```

## Question 3.4
```{r}
# Compute the probability of each pixels being in each class
predict_prob <- compute.prob.x.z(as.vector(I), result$pi, result$mu, result$sigma)

# Classify each pixels based on their pdf estimation
label <- rep(0, length(as.vector(I)))
for (i in 1:length(as.vector(I))) {
  label[i] <- which(predict_prob$p[i, ] == max(predict_prob$p[i, ]))
}

posterior <- predict_prob$p / rowSums(predict_prob$p)
mean <- t(replicate(length(as.vector(I)), result$mu))
posterior.mean <- rowSums(posterior * mean)

# Assign pixel values based on figure 3a color bar
label[label == 1] <- 0
label[label == 2] <- 8
label[label == 3] <- 4
```

## Question 3.5
```{r}
# Check dimension for reshape
dim(I)
# Plot figure 3a
image.plot(matrix(label, 398, 398))
```
```{r}
# Plot figure 3b
image.plot(matrix(posterior.mean, 398, 398))
```

